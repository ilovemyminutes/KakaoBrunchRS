{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from glob import glob\n",
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from load_data import load_user_time_read, load_raw\n",
    "from tfidf import TFIDFGenerator, get_df\n",
    "from config import Config\n",
    "from utils import iterate_data_files, squeeze, save_as_pickle\n",
    "from preprocessing import PostIdEncoder\n",
    "from utils import squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = list(map(lambda x: x[:-1], open('../raw/predict/dev.users', 'r').readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading time: 21.501490354537964\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "user_time_read = load_user_time_read(root_dir='../preprocessed/user_time_read.json')\n",
    "post_encoder = PostIdEncoder(root_dir='../encodings')\n",
    "tfidf_generator = TFIDFGenerator('../tfidf')\n",
    "print(f'Loading time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Getting user prefereces (2018100100-2019022200):   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   1%|          | 2/200 [00:00<00:12, 15.67it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   2%|▎         | 5/200 [00:00<00:12, 15.67it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   4%|▍         | 8/200 [00:00<00:11, 17.02it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   5%|▌         | 10/200 [00:00<00:12, 14.81it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   6%|▌         | 12/200 [00:01<00:19,  9.77it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   7%|▋         | 14/200 [00:02<00:42,  4.38it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):   8%|▊         | 16/200 [00:02<00:38,  4.76it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  10%|▉         | 19/200 [00:02<00:34,  5.22it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  11%|█         | 22/200 [00:02<00:25,  6.89it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  13%|█▎        | 26/200 [00:03<00:19,  9.10it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  14%|█▍        | 28/200 [00:03<00:16, 10.33it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  16%|█▌        | 31/200 [00:03<00:13, 12.24it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  17%|█▋        | 34/200 [00:03<00:15, 10.63it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  18%|█▊        | 36/200 [00:03<00:13, 11.76it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  19%|█▉        | 38/200 [00:03<00:13, 12.42it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  20%|██        | 41/200 [00:04<00:14, 10.72it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  22%|██▏       | 43/200 [00:04<00:23,  6.64it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  23%|██▎       | 46/200 [00:05<00:18,  8.27it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  24%|██▍       | 49/200 [00:05<00:14, 10.20it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  26%|██▌       | 51/200 [00:05<00:14, 10.13it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  28%|██▊       | 55/200 [00:05<00:11, 13.02it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  29%|██▉       | 58/200 [00:05<00:10, 13.10it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  30%|███       | 61/200 [00:05<00:09, 14.28it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  32%|███▏      | 63/200 [00:06<00:14,  9.76it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  32%|███▎      | 65/200 [00:06<00:12, 10.47it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  34%|███▎      | 67/200 [00:06<00:11, 11.71it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  34%|███▍      | 69/200 [00:06<00:15,  8.67it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  36%|███▌      | 71/200 [00:07<00:12, 10.26it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  36%|███▋      | 73/200 [00:07<00:12, 10.46it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  38%|███▊      | 76/200 [00:16<02:06,  1.02s/it]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  39%|███▉      | 78/200 [00:16<01:29,  1.36it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  41%|████      | 82/200 [00:16<01:01,  1.90it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  43%|████▎     | 86/200 [00:17<00:43,  2.65it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  44%|████▍     | 89/200 [00:17<00:32,  3.46it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  46%|████▌     | 92/200 [00:18<00:30,  3.58it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  48%|████▊     | 95/200 [00:18<00:24,  4.36it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  48%|████▊     | 97/200 [00:18<00:18,  5.68it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  50%|████▉     | 99/200 [00:18<00:14,  7.18it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  51%|█████     | 102/200 [00:18<00:10,  9.29it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  52%|█████▎    | 105/200 [00:18<00:08, 11.34it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  54%|█████▍    | 108/200 [00:19<00:08, 10.60it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  55%|█████▌    | 110/200 [00:19<00:08, 10.03it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  56%|█████▋    | 113/200 [00:19<00:06, 12.45it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  57%|█████▊    | 115/200 [00:19<00:06, 12.72it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  58%|█████▊    | 117/200 [00:20<00:09,  8.92it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  60%|█████▉    | 119/200 [00:20<00:09,  8.14it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  61%|██████    | 122/200 [00:20<00:08,  9.59it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  62%|██████▎   | 125/200 [00:20<00:06, 11.55it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  64%|██████▎   | 127/200 [00:20<00:05, 12.68it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  64%|██████▍   | 129/200 [00:21<00:06, 10.92it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  66%|██████▌   | 131/200 [00:21<00:05, 12.32it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  66%|██████▋   | 133/200 [00:21<00:05, 13.05it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  68%|██████▊   | 135/200 [00:21<00:05, 12.31it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  68%|██████▊   | 137/200 [00:21<00:07,  8.54it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  70%|███████   | 140/200 [00:22<00:05, 10.27it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  71%|███████   | 142/200 [00:22<00:05, 10.43it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  72%|███████▏  | 144/200 [00:22<00:05, 10.05it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  74%|███████▍  | 148/200 [00:22<00:04, 11.91it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  75%|███████▌  | 150/200 [00:22<00:03, 12.65it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  76%|███████▌  | 152/200 [00:23<00:04,  9.97it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  77%|███████▋  | 154/200 [00:23<00:04,  9.28it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  78%|███████▊  | 157/200 [00:23<00:05,  7.59it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  80%|████████  | 160/200 [00:24<00:04,  9.57it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  81%|████████  | 162/200 [00:24<00:03, 10.25it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  82%|████████▏ | 164/200 [00:25<00:06,  5.17it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  84%|████████▎ | 167/200 [00:25<00:05,  6.53it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  85%|████████▌ | 170/200 [00:25<00:03,  7.57it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  86%|████████▌ | 172/200 [00:25<00:03,  7.79it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  88%|████████▊ | 175/200 [00:26<00:03,  7.84it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  88%|████████▊ | 177/200 [00:26<00:02,  9.37it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  90%|█████████ | 180/200 [00:26<00:01, 10.12it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  92%|█████████▏| 183/200 [00:26<00:01, 12.37it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  94%|█████████▎| 187/200 [00:27<00:01, 10.63it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200):  96%|█████████▋| 193/200 [00:27<00:00, 13.90it/s]\u001b[A\n",
      "Getting user prefereces (2018100100-2019022200): 100%|██████████| 200/200 [00:27<00:00,  7.29it/s]Postprocessing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_read_by_time(history, start, end):\n",
    "    history_filtered = list(filter(lambda x: (int(start) <= int(x[0].split('_')[0])) and (int(end) >= int(x[0].split('_')[-1])), history))\n",
    "    return history_filtered\n",
    "\n",
    "start = Config.train_start\n",
    "end = Config.train_end\n",
    "n_splits = 5\n",
    "batch_size = len(dev) // n_splits\n",
    "post_meta_id = []\n",
    "\n",
    "for i in range(n_splits):\n",
    "    dev_user_batch = dev[i*batch_size:] if i == n_splits-1 else dev[i*batch_size:(i+1)*batch_size]\n",
    "    posts_raw = [] # 유저들의 로그에서 등장한 모든 글을 담을 리스트\n",
    "    user_preferences_raw = [] # 유저들의 feature 벡터를 담을 리스트\n",
    "\n",
    "    for user_id in tqdm(dev_user_batch, desc=f'Getting user prefereces ({start}-{end})'):\n",
    "        # 설정한 구간에 대한 해당 유저의 로그\n",
    "        history = filter_read_by_time(user_time_read[user_id], start, end) \n",
    "        history = squeeze(list(map(lambda x: x[-1], history)))\n",
    "\n",
    "        # 유저 로그로부터 TF-IDF 행렬 생성\n",
    "        user_tfidf = tfidf_generator.generate(post_encoder.transform(history), drop_id=False) # \n",
    "\n",
    "        # TF-IDF 행렬로부터 유저 feature 벡터를 생성\n",
    "        preference = sparse.csr_matrix(user_tfidf.iloc[:, 1:].values.sum(axis=0)[:, np.newaxis]) # post_meta_id 컬럼을 제외한 뒤 summation\n",
    "        user_tfidf = user_tfidf.groupby('post_meta_id').first().reset_index() # faster than drop_duplicates()\n",
    "        user_tfidf = user_tfidf.loc[~user_tfidf['post_meta_id'].isin(post_meta_id), :]\n",
    "        if len(user_tfidf) > 0:\n",
    "            post_meta_id.extend(user_tfidf['post_meta_id'].tolist())\n",
    "        posts_raw.append(sparse.csr_matrix(user_tfidf.iloc[:, 1:])) # post_meta_id 컬럼을 제외하고 append -> post_meta_id 리스트를 개별적으로 생성하므로 불필요\n",
    "        user_preferences_raw.append(preference)\n",
    "    \n",
    "    print('Postprocessing...')\n",
    "    posts = sparse.vstack(posts_raw)\n",
    "    user_preferences = sparse.hstack(user_preferences_raw)\n",
    "    idf = np.log(tfidf_generator.DF.values.squeeze()) - np.log((posts != 0).sum(axis=0) + 1e-4)\n",
    "# recommend_output = (posts.multiply(idf)).dot(user_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Split\n",
    "\n",
    "def filter_read_by_time(history, start, end):\n",
    "    history_filtered = list(filter(lambda x: (int(start) <= int(x[0].split('_')[0])) and (int(end) >= int(x[0].split('_')[-1])), history))\n",
    "    return history_filtered\n",
    "\n",
    "start = Config.train_start\n",
    "end = Config.train_end\n",
    "post_meta_id = []\n",
    "\n",
    "posts_raw = [] # 유저들의 로그에서 등장한 모든 글을 담을 리스트\n",
    "user_preferences_raw = [] # 유저들의 feature 벡터를 담을 리스트\n",
    "\n",
    "for user_id in tqdm(dev, desc=f'Getting user prefereces ({start}-{end})'):\n",
    "    # 설정한 구간에 대한 해당 유저의 로그\n",
    "    history = filter_read_by_time(user_time_read[user_id], start, end) \n",
    "    history = squeeze(list(map(lambda x: x[-1], history)))\n",
    "\n",
    "    # 유저 로그로부터 TF-IDF 행렬 생성\n",
    "    user_tfidf = tfidf_generator.generate(post_encoder.transform(history), drop_id=False) # \n",
    "\n",
    "    # TF-IDF 행렬로부터 유저 feature 벡터를 생성\n",
    "    preference = sparse.csr_matrix(user_tfidf.iloc[:, 1:].values.sum(axis=0)[:, np.newaxis]) # post_meta_id 컬럼을 제외한 뒤 summation\n",
    "    user_tfidf = user_tfidf.groupby('post_meta_id').first().reset_index() # faster than drop_duplicates()\n",
    "    user_tfidf = user_tfidf.loc[~user_tfidf['post_meta_id'].isin(post_meta_id), :]\n",
    "    if len(user_tfidf) > 0:\n",
    "        post_meta_id.extend(user_tfidf['post_meta_id'].tolist())\n",
    "    posts_raw.append(sparse.csr_matrix(user_tfidf.iloc[:, 1:])) # post_meta_id 컬럼을 제외하고 append -> post_meta_id 리스트를 개별적으로 생성하므로 불필요\n",
    "    user_preferences_raw.append(preference)\n",
    "\n",
    "print('Postprocessing...')\n",
    "posts = sparse.vstack(posts_raw)\n",
    "user_preferences = sparse.hstack(user_preferences_raw)\n",
    "idf = np.log(tfidf_generator.DF.values.squeeze()) - np.log((posts != 0).sum(axis=0) + 1e-4)\n",
    "# recommend_output = (posts.multiply(idf)).dot(user_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Extracting user prefereces based on 2018100100-2019022200: 0it [00:00, ?it/s]Getting User Preferences of Batch #0...\n",
      "Postprocessing...\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'post_meta_id'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'post_meta_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f3db1898b804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0muser_preferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_preferences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpost_meta_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post_meta_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_meta_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'post_meta_id'"
     ]
    }
   ],
   "source": [
    "def filter_read_by_time(history, start, end):\r\n",
    "    history_filtered = list(filter(lambda x: (int(start) <= int(x[0].split('_')[0])) and (int(end) >= int(x[0].split('_')[-1])), history))\r\n",
    "    return history_filtered\r\n",
    "\r\n",
    "start = Config.train_start\r\n",
    "end = Config.train_end\r\n",
    "n_splits = 25\r\n",
    "batch_size = len(dev) // n_splits\r\n",
    "seen_post_id = []\r\n",
    "\r\n",
    "for i in range(n_splits):\r\n",
    "    dev_user_batch = dev[i*batch_size:] if i == n_splits-1 else dev[i*batch_size:(i+1)*batch_size]\r\n",
    "    posts_raw = [] # 유저들의 로그에서 등장한 모든 글을 담을 리스트\r\n",
    "    user_preferences_raw = [] # 유저들의 feature 벡터를 담을 리스트\r\n",
    "\r\n",
    "    for user_id in tqdm(dev_user_batch, desc=f'Getting user prefereces ({start}-{end})'):\r\n",
    "        # 설정한 구간에 대한 해당 유저의 로그\r\n",
    "        history = filter_read_by_time(user_time_read[user_id], start, end) \r\n",
    "        history = squeeze(list(map(lambda x: x[-1], history)))\r\n",
    "\r\n",
    "        # 유저 로그로부터 TF-IDF 행렬 생성\r\n",
    "        user_tfidf = tfidf_generator.generate(post_encoder.transform(history), drop_id=False) # \r\n",
    "\r\n",
    "        # TF-IDF 행렬로부터 유저 feature 벡터를 생성\r\n",
    "        preference = user_tfidf.drop('post_meta_id', axis=1).values.sum(axis=0)\r\n",
    "        user_tfidf = user_tfidf.loc[~user_tfidf['post_meta_id'].isin(seen_post_id), :]\r\n",
    "        seen_post_id.extend(user_tfidf['post_meta_id'].tolist())\r\n",
    "\r\n",
    "        posts_raw.append(sparse.csr_matrix(user_tfidf))\r\n",
    "        user_preferences_raw.append(preference[:, np.newaxis])\r\n",
    "    \r\n",
    "    print('Postprocessing...')\r\n",
    "    # posts = pd.concat(posts_raw, axis=0, ignore_index=True).drop_duplicates(ignore_index=True) # 중복된 글은 제거\r\n",
    "    posts = sparse.vstack(posts_raw)\r\n",
    "    # post_meta_id = posts['post_meta_id'].tolist()\r\n",
    "    post_meta_id = posts[:,0].data.tolist()\r\n",
    "    posts = posts[:, 1:]\r\n",
    "    # posts = posts.drop('post_meta_id', axis=1)\r\n",
    "    user_preferences = np.hstack(user_preferences_raw)\r\n",
    "    # idf = np.log(tfidf_generator.DF.values.squeeze()) - np.log((posts != 0).sum().values + 1e-4)\r\n",
    "    idf = np.log(tfidf_generator.DF.values.squeeze()) - np.log((posts != 0).sum(axis=0) + 1e-4)\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Extracting user prefereces based on 2018100100-2019022200: 100%|██████████| 100/100 [46:26<00:00, 27.87s/it]\n",
      "Postprocessing...\n"
     ]
    }
   ],
   "source": [
    "def filter_read_by_time(history, start, end):\n",
    "    history_filtered = list(filter(lambda x: (int(start) <= int(x[0].split('_')[0])) and (int(end) >= int(x[0].split('_')[-1])), history))\n",
    "    return history_filtered\n",
    "\n",
    "def calculate_export_user_preferences(start: str=Config.train_start, end: str=Config.train_end, n_splits: int=50, save_path: str='./offline_tasks/user_preferences'):\n",
    "    print('Loading tools...', end='\\t')\n",
    "    dev_raw = [open('./preprocessed/train', 'r').readlines()]\n",
    "    dev_user_list = []\n",
    "    for daily in dev_raw:\n",
    "        users = [u.split()[0] for u in daily]\n",
    "        dev_user_list.extend(users)\n",
    "\n",
    "    user_time_read = load_user_time_read(root_dir='./preprocessed/user_time_read.json')\n",
    "    encoder = PostIdEncoder(root_dir='./encodings')\n",
    "    tfidf = TFIDFGenerator('./tfidf')\n",
    "    print('loaded!')\n",
    "\n",
    "    batch_size = len(dev_user_list) // n_splits\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        print(f'Getting User Preferences of Batch #{i}...')\n",
    "        if i == n_splits-1:\n",
    "            dev_user_batch = dev_user_list[i*batch_size:]\n",
    "        else:\n",
    "            dev_user_batch = dev_user_list[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        posts = pd.DataFrame()\n",
    "        user_preferences = np.zeros((7000, 1))\n",
    "\n",
    "        for user_id in tqdm(dev_user_batch, desc=f'Extracting user prefereces based on {start}-{end}'):\n",
    "            history = filter_read_by_time(user_time_read[user_id], start, end)\n",
    "            preference = np.zeros((1, 7000))\n",
    "            for h in history:\n",
    "                partial_tfidf = tfidf.generate(encoder.transform(h[-1]), drop_id=False)\n",
    "                preference += partial_tfidf.drop('post_meta_id', axis=1).values.sum(axis=0)\n",
    "                posts = pd.concat([posts, partial_tfidf], axis=0, ignore_index=True)\n",
    "            user_preferences = np.hstack([user_preferences, preference.reshape(7000, 1)])\n",
    "\n",
    "        print('Postprocessing...')\n",
    "        user_preferences = user_preferences[:, 1:]\n",
    "        posts = posts.drop_duplicates(ignore_index=True)\n",
    "        post_meta_id = posts['post_meta_id'].tolist()\n",
    "        posts = posts.drop('post_meta_id', axis=1)\n",
    "        idf = np.log(tfidf.DF.values.squeeze()) - np.log((posts != 0).sum().values + 1e-4)\n",
    "\n",
    "        print('Saving...')\n",
    "        batch_name = f'({start}-{end})batch{i+1:0>2d}'\n",
    "        os.mkdir(os.path.join(save_path, batch_name))\n",
    "        save_npz(os.path.join(save_path, batch_name, f'posts{i+1:0>2d}.npz'), csr_matrix(posts.values))\n",
    "        save_as_pickle(post_meta_id, os.path.join(save_path, batch_name, f'post_meta_id{i:0>2d}.pkl'))\n",
    "        np.save(os.path.join(save_path, batch_name, f'idf{i+1:0>2d}.npy'), idf)\n",
    "        np.save(os.path.join(save_path, batch_name, f'user_preferences{i+1:0>2d}.npy'), user_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_output = np.matmul((posts * idf).values, user_preferences)\n",
    "enc = PostIdEncoder(root_dir='../encodings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_recommend.npy', recommend_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_output_user = pd.DataFrame(dict(value=recommend_output[:, 1], id=post_meta_id)).sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_output_user['id'] = recommend_output_user['id'].apply(lambda x: enc.inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             value                   id\n",
       "39   322457.075150  @englishspeaking_66\n",
       "56   313143.167282         @linecard_45\n",
       "60   313143.167282         @linecard_43\n",
       "57   313143.167282         @linecard_42\n",
       "98   312260.888493         @linecard_77\n",
       "..             ...                  ...\n",
       "103  188480.532344           @hygo92_85\n",
       "82   184787.498120         @muncoach_25\n",
       "206  184734.588007        @thepiano_114\n",
       "131  184527.611553        @thepiano_106\n",
       "140  184499.304356           @nonie1_35\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>39</th>\n      <td>322457.075150</td>\n      <td>@englishspeaking_66</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>313143.167282</td>\n      <td>@linecard_45</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>313143.167282</td>\n      <td>@linecard_43</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>313143.167282</td>\n      <td>@linecard_42</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>312260.888493</td>\n      <td>@linecard_77</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>188480.532344</td>\n      <td>@hygo92_85</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>184787.498120</td>\n      <td>@muncoach_25</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>184734.588007</td>\n      <td>@thepiano_114</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>184527.611553</td>\n      <td>@thepiano_106</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>184499.304356</td>\n      <td>@nonie1_35</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "recommend_output_user.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}